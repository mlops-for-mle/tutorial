---
sidebar_position: 1
description: ğŸ“Œ Serving ì„ ìœ„í•œ data subscriber ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
---

# 1) Stream serving
import CodeDescription from '@site/src/components/CodeDescription';
import BrowserWindow from '@site/src/components/BrowserWindow';
import { Chapter, Part } from '@site/src/components/Highlight';

### ëª©í‘œ

1. Serving ì„ ìœ„í•œ data subscriber ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
2. Docker-compose ë¥¼ ì´ìš©í•˜ì—¬ data subcriber ë¥¼ ë„ì›ë‹ˆë‹¤.
3. Target DB ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œ inference í•œ ê²°ê³¼ê°€ ìŒ“ì´ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

<details>
<summary>ìŠ¤í™ ëª…ì„¸ì„œ</summary>
<CodeDescription>

### ìŠ¤í™ ëª…ì„¸ì„œ

1. Kafka Topic ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
2. ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ API Serving ì— ìš”ì²­ì„ ë³´ë‚´ ì˜ˆì¸¡ê°’ì„ ë°›ìŠµë‹ˆë‹¤.
3. ë°›ì•„ì˜¨ ì˜ˆì¸¡ê°’ì„ target postgres ì„œë²„ì— ì‚½ì…í•©ë‹ˆë‹¤.

</CodeDescription>
</details>

---

<BrowserWindow url="https://github.com/mlops-for-mle/mlops-for-mle/tree/main/ch8">

í•´ë‹¹ íŒŒíŠ¸ì˜ ì „ì²´ ì½”ë“œëŠ” [mlops-for-mle/ch8/](https://github.com/mlops-for-mle/mlops-for-mle/tree/main/ch8) ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
ch8
// highlight-next-line
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
// highlight-next-line
â”œâ”€â”€ data_subscriber.py
â”œâ”€â”€ grafana-docker-compose.yaml
// highlight-next-line
â””â”€â”€ stream-docker-compose.yaml
```

</BrowserWindow>

## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜

ì´ë²ˆ ì¥ì—ì„œ ì‚¬ìš©í•  íŒ¨í‚¤ì§€ë“¤ì…ë‹ˆë‹¤.

```bash
# terminal-command
pip install kafka-python requests psycopg2-binary
```
- `kafka-python` : python ì—ì„œ kafka ë¥¼ SDK í˜•íƒœë¡œ ì‚¬ìš©í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” kafka python client íŒ¨í‚¤ì§€ ì…ë‹ˆë‹¤. Consumer ë¥¼ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `requests` : python ìœ¼ë¡œ HTTP í†µì‹ ì´ í•„ìš”í•œ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤. API ë¥¼ í˜¸ì¶œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

## 1. Architecture

ì´ë²ˆ ì±•í„°ì—ì„œ êµ¬í˜„í•  ì„œë¹„ìŠ¤ë“¤ì€ [ê·¸ë¦¼ 8-2]ì™€ ê°™ìŠµë‹ˆë‹¤.

<div style={{textAlign: 'center'}}>

![Stream serving flow](./img/stream-2.png)
[ê·¸ë¦¼ 8-2] Stream serving flow
</div>

<Part>07. Kafka</Part> íŒŒíŠ¸ì—ì„œëŠ” source connector ì™€ sink connector ë¥¼ ìƒì„±í•˜ì—¬ source DB ì—ì„œ target DB ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë´¤ìŠµë‹ˆë‹¤.
<Part>08. Stream</Part> íŒŒíŠ¸ì—ì„œëŠ” ë‹¤ì‹œ model deployment ê´€ì ìœ¼ë¡œ ëŒì•„ì™€ì„œ kafka ë¥¼ ì–´ë–»ê²Œ ì“¸ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
<Part>07. Kafka</Part> íŒŒíŠ¸ì™€ ë™ì¼í•˜ê²Œ zookeeper, broker, connect ë¥¼ ì‚¬ìš©í•˜ë©° source connector ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ publish í•˜ëŠ” ê³¼ì •ë„ ë™ì¼í•©ë‹ˆë‹¤.
ë‹¬ë¼ì§€ëŠ” ì ì€ sink connector ë¥¼ ëŒ€ì‹ í•´ì„œ ì§ì ‘ kafka ì˜ python SDK ë¥¼ ì´ìš©í•˜ì—¬ consumer ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
<p></p>

<p></p>
ì™œ ì§ì ‘ êµ¬í˜„í•´ì„œ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?

<p></p>
<Part>07. Kafka</Part> ì—ì„œ ì‚¬ìš©í•œ sink connector ë¥¼ ì‚´í´ ë³´ë©´, JSON íŒŒì¼í‹€ í†µí•´ ì‚¬ì „ì— ì •ì˜ ëœ ì–‘ì‹ì— ë§ì¶”ì–´ ìƒì„±í•œ í›„ ì‚¬ìš©í•©ë‹ˆë‹¤.
í•˜ì§€ë§Œ <Part>08. Stream</Part> ì—ì„œëŠ” <Part>06. API serving</Part> ì—ì„œëŠ” request ë¥¼ ë³´ë‚´ê³  response ë¥¼ ë°›ëŠ” ê³¼ì •ì´ ì¡´ì¬í•©ë‹ˆë‹¤.
source DB ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ <Part>06. API serving</Part> ì—ì„œ ë§Œë“  REST API ë¡œ request ë¥¼ ë³´ë‚´ì£¼ì–´ì•¼í•˜ë©°, model ì˜ inference ê²°ê³¼ë¥¼ response ë¡œ ë°›ê²Œ ë©ë‹ˆë‹¤.
ë°›ì€ ê²°ê³¼ë¥¼ target DB ì— ì „ë‹¬í•˜ëŠ” ì´ ê³¼ì •ì„ ë‹´ë‹¹í•´ì£¼ëŠ” ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.
<p></p>

ë”°ë¼ì„œ ì´ë²ˆ ì±•í„°ì—ì„œëŠ” sink connector ì—†ì´ `kafka-python`, `requests`, `psycopg2` ë¥¼ ì´ìš©í•˜ì—¬ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•œ Consumer ë¥¼ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤. 


## 2. Data Subscriber

ì´ë²ˆì— êµ¬í˜„í•  data subscriber ì˜ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ Target DB ì— ì ‘ê·¼í•˜ì—¬ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
2. `kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ broker ì˜ topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ subscribe í•˜ëŠ” consumer ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3. `requests` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ consumer ë¥¼ í†µí•´ ë°›ì€ ë°ì´í„°ë¥¼ `API Serving` ì±•í„°ì—ì„œ ë„ìš´ API server ì— model ì˜ input ì¸ request ë¥¼ ë³´ë‚´ê³  model ì˜ inference ê²°ê³¼ì¸ response ë¥¼ ë°›ìŠµë‹ˆë‹¤.
4. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ ë°›ì€ response ë¥¼ target DB ì— ì¶”ê°€í•©ë‹ˆë‹¤.


### 2.1 í…Œì´ë¸” ìƒì„±

ë¨¼ì €, ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  í…Œì´ë¸” ìƒì„±ì„ í•˜ê² ìŠµë‹ˆë‹¤.  
ì „ë°˜ì ì¸ ì½”ë“œëŠ” <Part>01. Database</Part> íŒŒíŠ¸ì™€ ë™ì¼í•˜ë©° ë‹¤ë¥¸ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

<CodeDescription>

```python  title="data_subscriber.py"
import psycopg2

def create_table(db_connect):
    create_table_query = """
    CREATE TABLE IF NOT EXISTS iris_prediction (
        id SERIAL PRIMARY KEY,
        timestamp timestamp,
        iris_class int
    );"""
    print(create_table_query)
    with db_connect.cursor() as cur:
        cur.execute(create_table_query)
        db_connect.commit()

if __name__ == "__main__":
    db_connect = psycopg2.connect(
        user="targetuser",
        password="targetpassword",
        host="target-postgres-server",
        port=5432,
        database="targetdatabase",
    )
    create_table(db_connect)
```

- Connection
    - user : `targetuser`
    - password : `targetpassword`
    - host : `target-postgres-server`
    - port : `5432`
    - database : `targetdatabase`
- Table name : `iris_prediction`
- Schema : `id (PK)`, `timestamp (timestamp)`, `iris_class (int)`

</CodeDescription>

### 2.2 Consumer

ë‹¤ìŒìœ¼ë¡œ, consumer ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.  
`kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ `KafkaConsumer` ì˜ instance ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

<CodeDescription>

```python  title="data_subscriber.py"
from json import loads
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    "postgres-source-iris_data",
    bootstrap_servers="broker:29092",
    auto_offset_reset="earliest",
    enable_auto_commit=True,
    group_id="iris",
    value_deserializer=lambda x: loads(x.decode("utf-8")),
)
```

`KafkaConsumer` ì— ë“¤ì–´ê°€ëŠ” ì¸ìë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

- `topics` : ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ì‹¶ì€ topic ë“¤ì„ ë„£ìŠµë‹ˆë‹¤. ì´ë²ˆ ì±•í„°ì—ì„œëŠ” `postgres-source-iris_data` topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì´ê¸° ë•Œë¬¸ì— `postgres-source-iris_data` ì„ ì‘ì„±í•©ë‹ˆë‹¤.
- `bootstrap_servers` : bootstrap server ë¡œ ë„ì›Œì ¸ìˆëŠ” broker ì˜ <broker service name : broker service internal port\> ì„ ë„£ìŠµë‹ˆë‹¤. ì´ë²ˆ ì±•í„°ì—ì„œëŠ” `broker:29092` ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
- `auto_offset_reset` : topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì–´ë–¤ offset ê°’ë¶€í„° ê°€ì ¸ì˜¬ ì§€ ì„¤ì •í•©ë‹ˆë‹¤. 2ê°€ì§€ ì„¤ì •ì´ ìˆìœ¼ë©°, `earliest` ëŠ” ê°€ì¥ ì´ˆê¸° offset ê°’, `latest` ëŠ” ê°€ì¥ ë§ˆì§€ë§‰ offset ê°’ì…ë‹ˆë‹¤. ì´ë²ˆ ì±•í„°ì—ì„œëŠ” ì²«ë²ˆì§¸ ë°ì´í„°ë¶€í„° ê°€ì ¸ì˜¤ê³  ì‹¶ê¸° ë•Œë¬¸ì— `earliest` ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
- `group_id`  : consumer ê·¸ë£¹ì„ ì‹ë³„í•˜ê¸° ìœ„í•´ ê·¸ë£¹ ID ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì´ë²ˆ ì±•í„°ì—ì„œëŠ” `iris-data-consumer-group` ë¡œ ì§€ì •í•˜ê² ìŠµë‹ˆë‹¤.
- `value_deserializer` : source connector (ë˜ëŠ” produceer) ì—ì„œ serialization ëœ value ê°’ì„ deserialization í•  ë•Œ ì–´ë–¤ deserializer ë¥¼ ì‚¬ìš©í•  ì§€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    ì•ì„œ `Kafka` ì±•í„°ì—ì„œ  connect ë¥¼ ë„ìš¸ ë•Œ value converter ë¡œì„œ json converter ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë°ì´í„°ëŠ” json ìœ¼ë¡œ serialization ì´ ë˜ì–´ìˆìŠµë‹ˆë‹¤.
    ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ json deserializer ë¥¼ ì´ìš©í•˜ì—¬ deserialization ì„ í•´ì£¼ì–´ì•¼ í•¨ìœ¼ë¡œ, lambda function ê³¼ json ì˜ loads ë¥¼ ì´ìš©í•˜ì—¬ `lambda x: loads(x)` ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

</CodeDescription>

ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ consumer instance ëŠ” for ë¬¸ì„ ì´ìš©í•˜ì—¬ topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê³„ì†í•´ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python  title="data_subscriber.py"
for msg in consumer:
    print(
        f"Topic : {msg.topic}\n"
        f"Partition : {msg.partition}\n"
        f"Offset : {msg.offset}\n"
        f"Key : {msg.key}\n"
        f"Value : {msg.value}\n",
    )
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 133
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}}
# 
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 134
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 135, 'timestamp': '2022-12-15 04:49:42.27', 'sepal_length': 6.2, 'sepal_width': 2.9, 'petal_length': 4.3, 'petal_width': 1.3, 'target': 1}}
#
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 135
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 225, 'timestamp': '2022-12-15 04:51:14.238', 'sepal_length': 6.7, 'sepal_width': 3.1, 'petal_length': 4.4, 'petal_width': 1.4, 'target': 1}}
```

Print ë¬¸ê³¼ ì¶œë ¥ëœ í˜•íƒœë¥¼ í†µí•´ ë©”ì‹œì§€ì— ìˆëŠ” topic, partition, offset, key, value ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„°ëŠ” value ì— ìˆëŠ” `payload` í‚¤ ê°’ì…ë‹ˆë‹¤.

`payload` í‚¤ ê°’ì˜ í˜•íƒœë¥¼ ë³´ë©´ `'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}` ì™€ ê°™ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


### 2.3 API í˜¸ì¶œ

ë‹¤ìŒ ê³¼ì •ì€ ë°›ì•„ì§„ ë°ì´í„°ë¥¼ `API Serving` ì±•í„°ì—ì„œ ë„ì›Œì§„ API server ì— ì „ë‹¬í•˜ê³  model ì˜ inference ê²°ê³¼ë¥¼ ë°›ëŠ” ê²ƒì…ë‹ˆë‹¤.

`API Serving` ì±•í„°ì—ì„œ ë„ì›Œë‘” API server ì˜ schema ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. 

```python title="schemas.py"
from pydantic import BaseModel

class PredictIn(BaseModel):
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float

class PredictOut(BaseModel):
    iris_class: int
```

API server ì— request ë¡œ ì „ë‹¬í•  ê°’ë“¤ì—ëŠ” `sepal_length`, `sepal_width`, `petal_length`, `petal_width` ë§Œ ì „ë‹¬í•´ì•¼í•©ë‹ˆë‹¤.
ë”°ë¼ì„œ value ê°’ì˜ payload ì—ì„œ í•„ìš”ì—†ëŠ” column ë“¤ì€ ì‚­ì œë¥¼ í•´ì¤ë‹ˆë‹¤.

```python  title="data_subscriber.py"
msg.value["payload"].pop("id")
msg.value["payload"].pop("target")
ts = msg.value["payload"].pop("timestamp")
```

timestamp ì˜ ê²½ìš°, source DB ì—ì„œ ë‚˜ì˜¨ timestamp ë¥¼ target DB ì— ë„£ì–´ì¤„ ê²ƒì´ê¸° ë•Œë¬¸ì— ë˜‘ê°™ì´ ì‚­ì œëŠ” í•˜ë˜, `ts` ë³€ìˆ˜ë¡œ í• ë‹¹í•´ì¤ë‹ˆë‹¤.

ì´ì œ `requests` íŒ¨í‚¤ì§€ì— ìˆëŠ” POST request ë¥¼ ì´ìš©í•˜ì—¬ payload ê°’ë“¤ì„ ë³´ë‚´ê³  response ë¥¼ ë°›ìŠµë‹ˆë‹¤. 

<CodeDescription>

```python  title="data_subscriber.py"
response = requests.post(
    url="http://api-with-model:8000/predict",
    json=msg.value["payload"],
    headers={"Content-Type": "application/json"},
).json()
response["timestamp"] = ts
```

ë³´ë‚¼ ë•Œ ì“°ì´ëŠ” ì¸ìë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- `url` : API server ì˜ endpoint ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” API server ì˜ host name ê³¼ port number, ê·¸ë¦¬ê³  POST method ì¸ predict ë¥¼ í•©í•˜ì—¬ `"http://api-with-model:8000/predict"` ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.
- `json` : request ë¡œ ë³´ë‚¼ ì¸ìê°’ë“¤ì„ ëª…ì‹œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” value ê°’ì— ìˆëŠ” payload ê°’ì¸ `msg.value["payload"]` ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.
- `headers` : client ì—ì„œ server ë¡œ request ë¥¼ ë³´ë‚¼ ë•Œ ë¶€ê°€ì ì¸ ì •ë³´ë¥¼ ì „ì†¡í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë³´ë‚¼ ë•Œ json í˜•ì‹ìœ¼ë¡œ ë³´ë‚¼ ê²ƒì´ê¸° ë•Œë¬¸ì— `{"Content-Type": "application/json"}` header ë¡œ ì ì–´ì¤ë‹ˆë‹¤.

</CodeDescription>

Response ë¥¼ ë°›ê³  ë‚œ ë’¤ì— ì•„ê¹Œ ë‚¨ê²¨ë‘ì—ˆë˜ `ts` ë³€ìˆ˜ë¥¼ response ì— ë„£ì–´ì¤ë‹ˆë‹¤.

### 2.4 Target DB ì— ë°ì´í„° ì¶”ê°€

ë§ˆì§€ë§‰ìœ¼ë¡œ <Part>01. Database</Part> ì—ì„œ ì‚¬ìš©í–ˆë˜ `insert_data` method ë¥¼ ì´ìš©í•˜ì—¬ response ì— ë‹´ê¸´ ë°ì´í„°ë¥¼ target DB ì— ì¶”ê°€í•©ë‹ˆë‹¤.

```python  title="data_subscriber.py"
def insert_data(db_connect, data):
    insert_row_query = f"""
    INSERT INTO iris_prediction
        (timestamp, iris_class)
        VALUES (
            '{data["timestamp"]}',
            {data["iris_class"]}
        );"""
    print(insert_row_query)
    with db_connect.cursor() as cur:
        cur.execute(insert_row_query)
        db_connect.commit()

insert_data(db_connect, response)
```

### 2.5 ì „ì²´ ì½”ë“œ

ì•ì„œ ì‚´í´ë´¤ë˜ ëª¨ë“  ì½”ë“œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python title="data_subscriber.py"
from json import loads

import psycopg2
import requests
from kafka import KafkaConsumer

def create_table(db_connect):
    create_table_query = """
    CREATE TABLE IF NOT EXISTS iris_prediction (
        id SERIAL PRIMARY KEY,
        timestamp timestamp,
        iris_class int
    );"""
    print(create_table_query)
    with db_connect.cursor() as cur:
        cur.execute(create_table_query)
        db_connect.commit()

def insert_data(db_connect, data):
    insert_row_query = f"""
    INSERT INTO iris_prediction
        (timestamp, iris_class)
        VALUES (
            '{data["timestamp"]}',
            {data["iris_class"]}
        );"""
    print(insert_row_query)
    with db_connect.cursor() as cur:
        cur.execute(insert_row_query)
        db_connect.commit()

def subscribe_data(db_connect, consumer):
    for msg in consumer:
        print(
            f"Topic : {msg.topic}\n"
            f"Partition : {msg.partition}\n"
            f"Offset : {msg.offset}\n"
            f"Key : {msg.key}\n"
            f"Value : {msg.value}\n",
        )

        msg.value["payload"].pop("id")
        msg.value["payload"].pop("target")
        ts = msg.value["payload"].pop("timestamp")

        response = requests.post(
            url="http://api-with-model:8000/predict",
            json=msg.value["payload"],
            headers={"Content-Type": "application/json"},
        ).json()
        response["timestamp"] = ts
        insert_data(db_connect, response)

if __name__ == "__main__":
    db_connect = psycopg2.connect(
        user="targetuser",
        password="targetpassword",
        host="target-postgres-server",
        port=5432,
        database="targetdatabase",
    )
    create_table(db_connect)

    consumer = KafkaConsumer(
        "postgres-source-iris_data",
        bootstrap_servers="broker:29092",
        auto_offset_reset="earliest",
        group_id="iris-data-consumer-group",
        value_deserializer=lambda x: loads(x),
    )
    subscribe_data(db_connect, consumer)
```

## 3. Docker Compose

### 3.1
Data subscriber ì½”ë“œë¥¼ docker ì—ì„œ ì‹¤í–‰í•  `Dockerfile` ì…ë‹ˆë‹¤.

```docker  title="Dockerfile"
FROM amd64/python:3.9-slim

WORKDIR /usr/app

RUN pip install -U pip &&\
    pip install psycopg2-binary kafka-python requests

COPY data_subscriber.py data_subscriber.py

ENTRYPOINT ["python", "data_subscriber.py"]
```

### 3.2 Docker Compose

`Dockerfile` ì„ ì´ìš©í•˜ì—¬ `stream-docker-compose.yaml` ë¥¼ êµ¬ì„±í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```yaml title="stream-docker-compose.yaml"
version: "3"

services:
  data-subscriber:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data-subscriber

networks:
  default:
    name: mlops-network
    external: true
```
- ì„œë¹„ìŠ¤ë“¤ì„ ì—°ê²°í•  ë„ì»¤ ë„¤íŠ¸ì›Œí¬ë¥¼ <Part>01. Database íŒŒíŠ¸</Part> ì—ì„œ ìƒì„±í•œ mlops-network ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. `external: true` ì˜µì…˜ì€ `docker compose down -v` ë¡œ ì´ë²ˆ íŒŒíŠ¸ì—ì„œ ìƒì„±ë˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì¢…ë£Œí•˜ë”ë¼ë„ ìƒì„±ë˜ì–´ìˆëŠ” `mlops-network` ë¥¼ ì‚­ì œí•˜ì§€ ì•Šì„ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

### 3.3 ì‹¤í–‰

`docker compose` ëª…ë ¹ì–´ë¥¼ ì´ìš©í•˜ì—¬ data subscriber ì„œë¹„ìŠ¤ë¥¼ ë„ì›ë‹ˆë‹¤.

```bash
$ docker compose -p ch8-stream -f stream-docker-compose.yaml up -d
```

ì´ë•Œ `-p` ì¸ project name ì€ `ch8-stream` ìœ¼ë¡œ ì •í•˜ê³ , `-f` ì¸ file name ì€ ìœ„ì—ì„œ ì‘ì„±í•œ íŒŒì¼ ì´ë¦„ì„ ì ì–´ì¤ë‹ˆë‹¤.

## 3. ë°ì´í„° í™•ì¸

1. `psql` ë¡œ target DB ì— ì ‘ì†í•©ë‹ˆë‹¤.

    ```bash
    $ PGPASSWORD=targetpassword psql -h localhost -p 5433 -U targetuser -d targetdatabase
    psql (14.6 (Homebrew), server 14.0 (Debian 14.0-1.pgdg110+1))
    Type "help" for help.

    targetdatabase=#
    ```

2. Select ë¬¸ì„ ì‘ì„±í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    ```bash
    targetdatabase=# SELECT * FROM iris_data LIMIT 100;
    id  |        timestamp        | sepal_length | sepal_width | petal_length | petal_width | target 
    -----+-------------------------+--------------+-------------+--------------+-------------+--------
    1 | 2022-12-15 05:58:57.024 |          6.4 |         3.2 |          5.3 |         2.3 |      2
    2 | 2022-12-15 05:58:58.047 |          5.2 |         4.1 |          1.5 |         0.1 |      0
    3 | 2022-12-15 05:58:59.063 |          5.7 |         4.4 |          1.5 |         0.4 |      0
    4 | 2022-12-15 05:59:00.07  |          6.5 |           3 |          5.5 |         1.8 |      2
    5 | 2022-12-15 05:59:01.079 |          6.7 |         3.1 |          4.4 |         1.4 |      1
    6 | 2022-12-15 05:59:02.094 |          5.2 |         3.4 |          1.4 |         0.2 |      0
    7 | 2022-12-15 05:59:03.102 |          6.7 |           3 |            5 |         1.7 |      1
    8 | 2022-12-15 05:59:04.127 |          5.5 |         2.3 |            4 |         1.3 |      1
    9 | 2022-12-15 05:59:05.161 |          5.4 |         3.4 |          1.5 |         0.4 |      0
    ```
