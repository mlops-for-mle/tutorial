---
sidebar_position: 1
description: ğŸ“Œ Stream Serving ì„ ìœ„í•œ Data Subscriber ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
---

# 1) Stream Serving
import CodeDescription from '@site/src/components/CodeDescription';
import PreviewDescription from '@site/src/components/PreviewDescription';
import BrowserWindow from '@site/src/components/BrowserWindow';
import { Chapter, Part } from '@site/src/components/Highlight';

<PreviewDescription>

## Chapter Preview
---
### ëª©í‘œ

1. Stream Serving ì„ ìœ„í•œ Data Subscriber ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
2. Docker Compose ë¥¼ ì´ìš©í•˜ì—¬ Data Subcriber ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3. Target DB ì— ì ‘ì†í•˜ì—¬ ì˜ˆì¸¡ê°’ì´ ì˜ ìŒ“ì´ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

### ìŠ¤í™ ëª…ì„¸ì„œ

1. Stream Serving ì„ ìœ„í•œ Data Subscriber ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

    - 1. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  `iris_prediction` í…Œì´ë¸”ì„ Target DB ì„œë²„ì— ìƒì„±í•©ë‹ˆë‹¤.
    - 2. `kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ <Part>07. Kafka</Part> íŒŒíŠ¸ì—ì„œ ìƒì„±í•œ í† í”½ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” Consumer ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
            - <var>Topic name</var> : <code>postgres-source-iris_data</code>
            - <var>bootstrap_servers</var> : <code>broker:29092</code>
            - <var>auto_offset_reset</var> : <code>earliest</code>
            - <var>group_id</var> : <code>iris-data-consumer-group</code>
            - <var>value_deserializer</var> : <code>lambda x: loads(x)</code>
    - 3. `requests` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ REST API ì— ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ìš”ì²­í•˜ê³  ì˜ˆì¸¡ê°’ì„ ë°›ìŠµë‹ˆë‹¤.
    - 4. ê²°ê³¼ê°’ì„ `iris_prediction` í…Œì´ë¸”ì— ì‚½ì…í•©ë‹ˆë‹¤.
2. Docker Compose ë¥¼ ì´ìš©í•˜ì—¬ Data Subscriber ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3. Target DB ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œë¡œ ì˜ˆì¸¡ê°’ì´ ì˜ ìŒ“ì´ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

</PreviewDescription>

<BrowserWindow url="https://github.com/mlops-for-mle/mlops-for-mle/tree/main/ch8">

í•´ë‹¹ íŒŒíŠ¸ì˜ ì „ì²´ ì½”ë“œëŠ” [mlops-for-mle/ch8/](https://github.com/mlops-for-mle/mlops-for-mle/tree/main/ch8) ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```js
ch8
// highlight-next-line
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
// highlight-next-line
â”œâ”€â”€ data_subscriber.py
â”œâ”€â”€ grafana-docker-compose.yaml
// highlight-next-line
â””â”€â”€ stream-docker-compose.yaml
```

</BrowserWindow>

## 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜

ì´ë²ˆ ì±•í„°ì—ì„œ ì‚¬ìš©í•  íŒ¨í‚¤ì§€ë“¤ì…ë‹ˆë‹¤.

```bash
# terminal-command
pip install kafka-python requests psycopg2-binary
```
- <var>kafka-python</var> : 

    - Python ì—ì„œ Kafka ë¥¼ SDK í˜•íƒœë¡œ ì‚¬ìš©í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” Kafka Python Client íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤.
    - í•´ë‹¹ ì±•í„°ì—ì„œëŠ” Consumer ë¥¼ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- <var>requests</var> : 

    - Python ìœ¼ë¡œ HTTP í†µì‹ ì´ í•„ìš”í•œ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤. 
    - REST API ë¥¼ í˜¸ì¶œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

## 1. Architecture

ì´ë²ˆ ì±•í„°ì—ì„œ êµ¬í˜„í•  ì„œë¹„ìŠ¤ë“¤ì€ [ê·¸ë¦¼ 8-2]ì™€ ê°™ìŠµë‹ˆë‹¤.

<div style={{textAlign: 'center'}}>

![Stream serving flow](./img/stream-2.png)
[ê·¸ë¦¼ 8-2] Stream Serving Workflow
</div>
<p>
<Part>07. Kafka</Part> íŒŒíŠ¸ì—ì„œëŠ” Source Connector ì™€ Sink Connector ë¥¼ ìƒì„±í•˜ì—¬ Source DB ì—ì„œ Target DB ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë´¤ìŠµë‹ˆë‹¤.
<Part>08. Stream</Part> íŒŒíŠ¸ì—ì„œëŠ” ë‹¤ì‹œ Model Deployment ê´€ì ìœ¼ë¡œ ëŒì•„ì™€ì„œ Kafka ë¥¼ ì–´ë–»ê²Œ ì“¸ ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.<br></br>
</p>
<p>
ì´ë²ˆ íŒŒíŠ¸ì—ì„œëŠ” <Part>07. Kafka</Part> íŒŒíŠ¸ì™€ ë™ì¼í•œ ì£¼í‚¤í¼, ë¸Œë¡œì»¤, Connect, Schema Registry ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
ë˜í•œ Source Connector ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê³¼ì •ë„ ë™ì¼í•©ë‹ˆë‹¤.
ë‹¬ë¼ì§€ëŠ” ì ì€ Sink Connector ë¥¼ ëŒ€ì‹ í•´ì„œ ì§ì ‘ Kafka Python SDK ë¥¼ ì´ìš©í•˜ì—¬ Consumer ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
</p>
<p>
ì™œ ì§ì ‘ êµ¬í˜„í•´ì„œ ì‚¬ìš©í•´ì•¼ í• ê¹Œìš”?<br></br>
<Part>07. Kafka</Part> íŒŒíŠ¸ì—ì„œ ì‚¬ìš©í•œ Sink Connector ë¥¼ ì‚´í´ ë³´ë©´, ì„¤ì • íŒŒì¼ì„ í†µí•´ ìƒì„±í•œ í›„ ìë™ìœ¼ë¡œ Sink Connector ê°€ í•´ë‹¹ í† í”½ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì½ì–´ì„œ Target DB ì— ì „ë‹¬í–ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ <Part>08. Stream</Part> íŒŒíŠ¸ì—ì„œëŠ” <Part>06. API serving</Part> íŒŒíŠ¸ì—ì„œ ìƒì„±í•œ API ì„œë²„ì— request ë¥¼ ë³´ë‚´ê³  response ë¥¼ ë°›ì•„ì•¼í•©ë‹ˆë‹¤.
ì´ ê³¼ì •ì—ì„œ Sink Connector ë¥¼ ì“°ë ¤ë©´ í† í”½ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì„œ ì „ë‹¬í•  endpoint ê°€ í•„ìš”í•˜ì§€ë§Œ, API Serving ì—ì„œëŠ” ìˆ˜ë™ìœ¼ë¡œ request ë¥¼ ë³´ë‚´ê³  response ë¥¼ ë°›ê¸° ë•Œë¬¸ì— Sink Connector ë¥¼ ì‚¬ìš©í•  ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.
</p>
<p>
ë”°ë¼ì„œ Source DB ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ API ì„œë²„ë¡œ ìš”ì²­ì„ ë³´ë‚´ê³ , ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ì„ ë°›ê³ , ë°›ì€ ê²°ê³¼ë¥¼ Target DB ì— ì‚½ì…í•˜ëŠ” ê³¼ì •ì„ ë‹´ë‹¹í•˜ëŠ” ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.
</p>

ë”°ë¼ì„œ ì´ë²ˆ ì±•í„°ì—ì„œëŠ” Sink Connector ì—†ì´ `kafka-python`, `requests`, `psycopg2` íŒ¨í‚¤ì§€ë“¤ì„ ì´ìš©í•˜ì—¬ Data Subscriber ë¥¼ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤. 


## 2. Data Subscriber

ì´ë²ˆì— êµ¬í˜„í•  Data Subscriber ì˜ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ Target DB ì— ì ‘ê·¼í•˜ì—¬ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
2. `kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ ë¸Œë¡œì»¤ì˜ í† í”½ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì½ëŠ” Consumer ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3. `requests` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ Consumer ë¥¼ í†µí•´ ë°›ì€ ë°ì´í„°ë¥¼ <Part>06. API serving</Part> íŒŒíŠ¸ì—ì„œ ë„ìš´ API ì„œë²„ì— ë°ì´í„°ë¥¼ ë³´ë‚´ê³  ì˜ˆì¸¡ê°’ì„ ë°›ìŠµë‹ˆë‹¤.
4. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ ë°›ì€ response ë¥¼ Target DB ì— ì‚½ì…í•©ë‹ˆë‹¤.


### 2.1 Prediction í…Œì´ë¸” ìƒì„±

ë¨¼ì €, ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
ì „ë°˜ì ì¸ ì½”ë“œëŠ” <Part>01. Database</Part> íŒŒíŠ¸ì™€ ë™ì¼í•˜ë©° ë‹¤ë¥¸ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

<CodeDescription>

```python  title="data_subscriber.py"
import psycopg2

def create_table(db_connect):
    create_table_query = """
    CREATE TABLE IF NOT EXISTS iris_prediction (
        id SERIAL PRIMARY KEY,
        timestamp timestamp,
        iris_class int
    );"""
    print(create_table_query)
    with db_connect.cursor() as cur:
        cur.execute(create_table_query)
        db_connect.commit()

if __name__ == "__main__":
    db_connect = psycopg2.connect(
        user="targetuser",
        password="targetpassword",
        host="target-postgres-server",
        port=5432,
        database="targetdatabase",
    )
    create_table(db_connect)
```

- <var>Connection</var> :

    - <var>user</var> : <code>targetuser</code>
    - <var>password</var> : <code>targetpassword</code>
    - <var>host</var> : <code>target-postgres-server</code>
    - <var>port</var> : <code>5432</code>
    - <var>database</var> : <code>targetdatabase</code>
- <var>Table name</var> : <code>iris_prediction</code>
- <var>Schema</var> : 

    - <code>id (PK)</code>, <code>timestamp (timestamp)</code>, <code>iris_class (int)</code>

</CodeDescription>

### 2.2 Consumer ìƒì„±

ë‹¤ìŒìœ¼ë¡œ, Consumer ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.  
`kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ `KafkaConsumer` ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤.

<CodeDescription>

```python  title="data_subscriber.py"
from json import loads
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    "postgres-source-iris_data",
    bootstrap_servers="broker:29092",
    auto_offset_reset="earliest",
    group_id="iris-data-consumer-group",
    value_deserializer=lambda x: loads(x),
)
```
- <var>topics</var>:

    - ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ê³  ì‹¶ì€ í† í”½ì„ ì„¤ì •í•©ë‹ˆë‹¤.
- <var>bootstrap_servers</var> :

    - Bootstrap ì„œë²„ë¡œ ë„ì›Œì ¸ìˆëŠ” ë¸Œë¡œì»¤ì˜ <code>ë¸Œë¡œì»¤ ì„œë¹„ìŠ¤ ì´ë¦„ : ë¸Œë¡œì»¤ ì„œë¹„ìŠ¤ ë‚´ë¶€ í¬íŠ¸</code> ì„ ë„£ìŠµë‹ˆë‹¤.
- <var>auto_offset_reset</var> :

    - í† í”½ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì–´ë–¤ offset ê°’ë¶€í„° ê°€ì ¸ì˜¬ ì§€ ì„¤ì •í•©ë‹ˆë‹¤. 
    - 2ê°€ì§€ ì„¤ì •ì´ ìˆìœ¼ë©°, <code>earliest</code> ëŠ” ê°€ì¥ ì´ˆê¸° offset ê°’, <code>latest</code> ëŠ” ê°€ì¥ ë§ˆì§€ë§‰ offset ê°’ì…ë‹ˆë‹¤.
    - ì´ë²ˆ ì±•í„°ì—ì„œëŠ” ì²«ë²ˆì§¸ ë°ì´í„°ë¶€í„° ê°€ì ¸ì˜¤ê³  ì‹¶ê¸° ë•Œë¬¸ì— <code>earliest</code> ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
- <var>group_id</var> : 

    - Consumer ê·¸ë£¹ì„ ì‹ë³„í•˜ê¸° ìœ„í•´ ê·¸ë£¹ ID ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
- <var>value_deserializer</var> :

    - Source connector (ë˜ëŠ” Produceer) ì—ì„œ serialization ëœ value ê°’ì„ deserialization í•  ë•Œ ì‚¬ìš©í•  deserializer ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    - <Part>07. Kafka</Part>íŒŒíŠ¸ì—ì„œëŠ” Connect ë¥¼ ë„ìš¸ ë•Œ value converter ë¡œì„œ Json Converter ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë°ì´í„°ëŠ” json ìœ¼ë¡œ serialization ì´ ë˜ì–´ìˆìŠµë‹ˆë‹¤.
    - ì´ë²ˆ ì±•í„°ì—ì„œëŠ” ë°ì´í„°ë¥¼ ì½ì–´ì„œ Json Deserializer ë¥¼ ì´ìš©í•˜ì—¬ deserialization ì„ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— lambda function ê³¼ json ì˜ <code>loads</code> ë¥¼ ì´ìš©í•˜ì—¬ <code>lambda x: loads(x)</code> ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

</CodeDescription>

ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ Consumer ì¸ìŠ¤í„´ìŠ¤ëŠ” for ë¬¸ì„ ì´ìš©í•˜ì—¬ í† í”½ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê³„ì†í•´ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python  title="data_subscriber.py"
for msg in consumer:
    print(
        f"Topic : {msg.topic}\n"
        f"Partition : {msg.partition}\n"
        f"Offset : {msg.offset}\n"
        f"Key : {msg.key}\n"
        f"Value : {msg.value}\n",
    )
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 133
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}}
# 
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 134
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 135, 'timestamp': '2022-12-15 04:49:42.27', 'sepal_length': 6.2, 'sepal_width': 2.9, 'petal_length': 4.3, 'petal_width': 1.3, 'target': 1}}
#
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 135
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 225, 'timestamp': '2022-12-15 04:51:14.238', 'sepal_length': 6.7, 'sepal_width': 3.1, 'petal_length': 4.4, 'petal_width': 1.4, 'target': 1}}
```

Print ë¬¸ê³¼ ì¶œë ¥ëœ í˜•íƒœë¥¼ í†µí•´ ë©”ì‹œì§€ì— ìˆëŠ” topic, partition, offset, key, value ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„°ëŠ” value ì— ìˆëŠ” payload ê°’ì…ë‹ˆë‹¤.

payload ê°’ì˜ í˜•íƒœë¥¼ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```json
'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}
``` 


### 2.3 API í˜¸ì¶œ

ë‹¤ìŒ ê³¼ì •ì€ ì½ì–´ë“œë¦° ë°ì´í„°ë¥¼ <Part>06. API Serving</Part> íŒŒíŠ¸ì—ì„œ ìƒì„±í•œ API ì„œë²„ì— ì „ë‹¬í•˜ê³ , ì˜ˆì¸¡ê°’ì„ ë°›ëŠ” ê²ƒì…ë‹ˆë‹¤.

#### 2.3.1 Schema í™•ì¸

ë¨¼ì € <Part>06. API Serving</Part> íŒŒíŠ¸ì—ì„œ ë„ì›Œë‘” API ì„œë²„ì˜ schema ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. 

```python title="schemas.py"
from pydantic import BaseModel

class PredictIn(BaseModel):
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float

class PredictOut(BaseModel):
    iris_class: int
```

API ì„œë²„ì— request ë¡œ ë³´ë‚¼ ê°’ë“¤ì—ëŠ” `sepal_length`, `sepal_width`, `petal_length`, `petal_width` column ì´ ìˆê³ , response ë¡œ ë°›ëŠ” ê°’ë“¤ì—ëŠ” `iris_class` column ì´ ìˆìŠµë‹ˆë‹¤.

#### 2.3.2 API ìš”ì²­ ë° ì‘ë‹µ

API ì„œë²„ì— request ë¡œ ë³´ë‚¼ ê°’ë“¤ ì¤‘ payload ì—ì„œ í•„ìš”ì—†ëŠ” column ë“¤ì„ ì•„ë˜ì™€ ê°™ì´ ì‚­ì œí•©ë‹ˆë‹¤.

```python  title="data_subscriber.py"
msg.value["payload"].pop("id")
msg.value["payload"].pop("target")
ts = msg.value["payload"].pop("timestamp")
```

timestamp ì˜ ê²½ìš°, Source DB ì—ì„œ ë‚˜ì˜¨ timestamp ë¥¼ Target DB ì— ë„£ì–´ì¤„ ê²ƒì´ê¸° ë•Œë¬¸ì— ë˜‘ê°™ì´ ì‚­ì œëŠ” í•˜ë˜, `ts` ë³€ìˆ˜ë¡œ í• ë‹¹í•´ì¤ë‹ˆë‹¤.

ì´ì œ `requests` íŒ¨í‚¤ì§€ì— ìˆëŠ” POST method ë¥¼ ì´ìš©í•˜ì—¬ payload ê°’ë“¤ì„ ë³´ë‚´ê³  response ë¥¼ ë°›ìŠµë‹ˆë‹¤. 

<CodeDescription>

```python  title="data_subscriber.py"
response = requests.post(
    url="http://api-with-model:8000/predict",
    json=msg.value["payload"],
    headers={"Content-Type": "application/json"},
).json()
response["timestamp"] = ts
```

- <var>url</var> : 

    - API ì„œë²„ì˜ endpoint ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    - ì´ë²ˆ ì±•í„°ì—ì„œëŠ” API ì„œë²„ì˜ í˜¸ìŠ¤íŠ¸ ì´ë¦„ê³¼ í¬íŠ¸, ê·¸ë¦¬ê³  POST method ì¸ predict ë¥¼ í•©í•˜ì—¬ <code>http://api-with-model:8000/predict</code> ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.
- <var>json</var> : 

    - request ë¡œ ë³´ë‚¼ ì¸ìê°’ë“¤ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
    - ì´ë²ˆ ì±•í„°ì—ì„œëŠ” payload ê°’ì¸ `msg.value["payload"]` ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.
- <var>headers</var> : 

    - Client ì—ì„œ Server ë¡œ request ë¥¼ ë³´ë‚¼ ë•Œ ë¶€ê°€ì ì¸ ì •ë³´ë¥¼ ì „ì†¡í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
    - ì´ë²ˆ ì±•í„°ì—ì„œëŠ” ë³´ë‚¼ ë•Œ json í˜•ì‹ìœ¼ë¡œ ë³´ë‚¼ ê²ƒì´ê¸° ë•Œë¬¸ì— <code>{"Content-Type": "application/json"}</code> header ë¡œ ì ì–´ì¤ë‹ˆë‹¤.
- Response ë¥¼ ë°›ê³  ë‚œ ë’¤ì— ì•„ê¹Œ ë‚¨ê²¨ë‘ì—ˆë˜ `ts` ë³€ìˆ˜ë¥¼ response ì— ë„£ì–´ì¤ë‹ˆë‹¤.

</CodeDescription>


### 2.4 Prediction í…Œì´ë¸”ì— ì˜ˆì¸¡ê°’ ì‚½ì…

ë§ˆì§€ë§‰ìœ¼ë¡œ <Part>01. Database</Part> íŒŒíŠ¸ì—ì„œ ì‚¬ìš©í–ˆë˜ `insert_data` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ response ì— ë‹´ê¸´ ë°ì´í„°ë¥¼ Target DB ì— ì‚½ì…í•©ë‹ˆë‹¤.

```python  title="data_subscriber.py"
def insert_data(db_connect, data):
    insert_row_query = f"""
    INSERT INTO iris_prediction
        (timestamp, iris_class)
        VALUES (
            '{data["timestamp"]}',
            {data["iris_class"]}
        );"""
    print(insert_row_query)
    with db_connect.cursor() as cur:
        cur.execute(insert_row_query)
        db_connect.commit()

insert_data(db_connect, response)
```

### 2.5 `data_subscriber.py`

ì•ì„œ ì‚´í´ë´¤ë˜ ëª¨ë“  ì½”ë“œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python title="data_subscriber.py"
# data_subscriber.py
from json import loads

import psycopg2
import requests
from kafka import KafkaConsumer


def create_table(db_connect):
    create_table_query = """
    CREATE TABLE IF NOT EXISTS iris_prediction (
        id SERIAL PRIMARY KEY,
        timestamp timestamp,
        iris_class int
    );"""
    print(create_table_query)
    with db_connect.cursor() as cur:
        cur.execute(create_table_query)
        db_connect.commit()


def insert_data(db_connect, data):
    insert_row_query = f"""
    INSERT INTO iris_prediction
        (timestamp, iris_class)
        VALUES (
            '{data["timestamp"]}',
            {data["iris_class"]}
        );"""
    print(insert_row_query)
    with db_connect.cursor() as cur:
        cur.execute(insert_row_query)
        db_connect.commit()


def subscribe_data(db_connect, consumer):
    for msg in consumer:
        print(
            f"Topic : {msg.topic}\n"
            f"Partition : {msg.partition}\n"
            f"Offset : {msg.offset}\n"
            f"Key : {msg.key}\n"
            f"Value : {msg.value}\n",
        )

        msg.value["payload"].pop("id")
        msg.value["payload"].pop("target")
        ts = msg.value["payload"].pop("timestamp")

        response = requests.post(
            url="http://api-with-model:8000/predict",
            json=msg.value["payload"],
            headers={"Content-Type": "application/json"},
        ).json()
        response["timestamp"] = ts
        insert_data(db_connect, response)


if __name__ == "__main__":
    db_connect = psycopg2.connect(
        user="targetuser",
        password="targetpassword",
        host="target-postgres-server",
        port=5432,
        database="targetdatabase",
    )
    create_table(db_connect)

    consumer = KafkaConsumer(
        "postgres-source-iris_data",
        bootstrap_servers="broker:29092",
        auto_offset_reset="earliest",
        group_id="iris-data-consumer-group",
        value_deserializer=lambda x: loads(x),
    )
    subscribe_data(db_connect, consumer)
```

## 3. Docker Compose

### 3.1 Dockerfile

Data Subscriber ì½”ë“œë¥¼ Docker ì—ì„œ ì‹¤í–‰í•  Dockerfile ì„ ë§Œë“­ë‹ˆë‹¤.

```docker  title="Dockerfile"
FROM amd64/python:3.9-slim

WORKDIR /usr/app

RUN pip install -U pip &&\
    pip install psycopg2-binary kafka-python requests

COPY data_subscriber.py data_subscriber.py

ENTRYPOINT ["python", "data_subscriber.py"]
```

### 3.2 Docker Compose

Dockerfile ì„ ì´ìš©í•˜ì—¬ Docker Compose íŒŒì¼ì„ êµ¬ì„±í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```yaml title="stream-docker-compose.yaml"
# stream-docker-compose.yaml
version: "3"

services:
  data-subscriber:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data-subscriber

networks:
  default:
    name: mlops-network
    external: true
```

- ì„œë¹„ìŠ¤ë“¤ì„ ì—°ê²°í•  Docker Network ëŠ” <Part>01. Database</Part> íŒŒíŠ¸ì—ì„œ ìƒì„±í•œ `mlops-network` ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

### 3.3 ì‹¤í–‰

`docker compose` ëª…ë ¹ì–´ë¥¼ ì´ìš©í•˜ì—¬ Data Subscriber ì„œë¹„ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

<CodeDescription>

```bash
# terminal-command
docker compose -p ch8-stream -f stream-docker-compose.yaml up -d
```

- <var>-p</var> : 

  - Project name ì€ <code>ch8-stream</code> ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- <var>-f</var> :

  - File name ì€ ìœ„ì—ì„œ ì‘ì„±í•œ íŒŒì¼ ì´ë¦„ì¸ <code>stream-docker-compose.yaml</code> ì„ ì ì–´ì¤ë‹ˆë‹¤.

</CodeDescription>

### 3.4 ë°ì´í„° í™•ì¸

1. `psql` ë¡œ Target DB ì— ì ‘ì†í•©ë‹ˆë‹¤.

    ```bash
    $ PGPASSWORD=targetpassword psql -h localhost -p 5433 -U targetuser -d targetdatabase
    psql (14.6, server 14.0 (Debian 14.0-1.pgdg110+1))
    Type "help" for help.

    targetdatabase=#
    ```

2. Select ë¬¸ì„ ì‘ì„±í•˜ì—¬ `iris_prediction` í…Œì´ë¸”ì— ìˆëŠ” ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

    ```bash
    targetdatabase=# SELECT * FROM iris_prediction LIMIT 100;
     id  |        timestamp        | iris_class 
    -----+-------------------------+------------
       1 | 2022-12-21 23:31:12.705 |          1
       2 | 2022-12-21 23:31:13.804 |          2
       3 | 2022-12-21 23:31:14.815 |          2
       4 | 2022-12-21 23:31:15.828 |          2
       5 | 2022-12-21 23:31:16.835 |          1
       6 | 2022-12-21 23:31:17.848 |          1
       7 | 2022-12-21 23:31:18.854 |          1
       8 | 2022-12-21 23:31:19.863 |          0
       9 | 2022-12-21 23:31:20.875 |          2
    ```
