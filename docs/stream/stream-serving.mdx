---
sidebar_position: 1
description: ğŸ“Œ Stream serving
---

# 1) Stream serving

### ëª©í‘œ

1. Serving ì„ ìœ„í•œ data subscriber ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
2. Docker-compose ë¥¼ ì´ìš©í•˜ì—¬ data subcriber ë¥¼ ë„ì›ë‹ˆë‹¤.
3. Target DB ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œ inference í•œ ê²°ê³¼ê°€ ìŒ“ì´ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

### ìŠ¤í™ ëª…ì„¸ì„œ

1. TBD

---

## 1. ì „ì²´êµ¬ì¡°

ì´ì „ ì±•í„°ì—ì„œëŠ” ìµœì¢…ì ìœ¼ë¡œ source connector ì™€ sink connector ë¥¼ ìƒì„±í•˜ì—¬ source DB ì—ì„œ target DB ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë´¤ìŠµë‹ˆë‹¤. ì´ë²ˆ ì±•í„°ì—ì„œëŠ” ë‹¤ì‹œ model deployment ê´€ì ìœ¼ë¡œ ëŒì•„ì™€ì„œ kafka ë¥¼ ì–´ë–»ê²Œ ì“¸ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ì´ì „ê³¼ ë™ì¼í•˜ê²Œ zookeeper, broker, connect ë¥¼ ì“°ëŠ” ê²ƒì€ ë™ì¼í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  source connector ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ publish í•˜ëŠ” ê³¼ì •ë„ ë™ì¼í•©ë‹ˆë‹¤. ë‹¬ë¼ì§€ëŠ” ì ì€ sink connector ë¥¼ ì œì™¸í•˜ê³  ì§ì ‘ kafka ì˜ python SDK ë¥¼ ì´ìš©í•˜ì—¬ consumer ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì™œ ì§ì ‘ êµ¬í˜„í•´ì„œ ì‚¬ìš©í•´ì•¼í• ê¹Œìš”? ì´ìœ ë¥¼ ì‚´í´ë³´ìë©´, API serving ì—ì„œëŠ” request ë¥¼ ë³´ë‚´ê³  response ë¥¼ ë°›ëŠ” ê³¼ì •ì´ ì¡´ì¬í•©ë‹ˆë‹¤. sink connector ë¥¼ ì“°ë ¤ë©´ ìµœì¢…ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬í•  endpoint ê°€ ì¡´ì¬í•´ì•¼í•˜ëŠ”ë° API serving ì—ì„œëŠ” ìë™ìœ¼ë¡œ API ë¥¼ ê±°ì³ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤. API serving ì—ì„œëŠ” source DB ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ request ë¥¼ ë³´ë‚´ì£¼ì–´ì•¼í•˜ë©°, model ì˜ inference ê²°ê³¼ë¥¼ response ë¡œ ë°›ê²Œ ë©ë‹ˆë‹¤. ë°›ì€ ê²°ê³¼ë¥¼ target DB ì— ì „ë‹¬í•˜ëŠ” ì´ ê³¼ì •ì„ ìˆ˜ë™ì ìœ¼ë¡œ ë‹´ë‹¹í•´ì£¼ëŠ” ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ì´ë²ˆ ì±•í„°ì—ì„œëŠ” sink connector ì—†ì´ `kafka-python` , `requests` , `psycopg2` ë¥¼ ì´ìš©í•˜ì—¬ ìˆ˜ë™ì ì¸ ê³¼ì •ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤. 

![image.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8acf2bd5-8afa-45e4-bf7e-624989bda0c7/image.png)

docker-compose íŒŒì¼ì— ì‘ì„±ë  ì„œë¹„ìŠ¤ë“¤ì˜ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤. ì´ë²ˆ ì¥ì—ì„œëŠ” serving ì„ ìœ„í•œ data subscriber ë¥¼ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

## 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜

ì´ë²ˆ ì¥ì—ì„œ ì‚¬ìš©í•  íŒ¨í‚¤ì§€ë“¤ì…ë‹ˆë‹¤.

- `kafka-python` : python ì—ì„œ kafka ë¥¼ SDK í˜•íƒœë¡œ ì‚¬ìš©í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” kafka python client íŒ¨í‚¤ì§€ ì…ë‹ˆë‹¤. Consumer ë¥¼ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `requests` : python ìœ¼ë¡œ HTTP í†µì‹ ì´ í•„ìš”í•œ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•  ë•Œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤. API ë¥¼ í˜¸ì¶œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `psycopg2-binary` : ì±•í„° 1ì—ì„œ ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼ python ì—ì„œ DB ì— ì ‘ê·¼í•  ë•Œ ì‚¬ìš©í•˜ëŠ” íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤. API ì—ì„œ ë°›ì€ response ê²°ê³¼ë¥¼ target DB ì— ì¶”ê°€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

ìœ„ì˜ íŒ¨í‚¤ì§€ë“¤ì€ Dockerfile ì—ì„œ ì„¤ì¹˜í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ë”°ë¡œ local ì—ì„œ ì„¤ì¹˜í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

## 2. Data Subscriber

ì´ë²ˆì— êµ¬í˜„í•  data subscriber ì˜ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ Target DB ì— ì ‘ê·¼í•˜ì—¬ í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤.
2. `kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ broker ì˜ topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ subscribe í•˜ëŠ” consumer ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
3. `requests` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ consumer ë¥¼ í†µí•´ ë°›ì€ ë°ì´í„°ë¥¼ `API Serving` ì±•í„°ì—ì„œ ë„ìš´ API server ì— model ì˜ input ì¸ request ë¥¼ ë³´ë‚´ê³  model ì˜ inference ê²°ê³¼ì¸ response ë¥¼ ë°›ìŠµë‹ˆë‹¤.
4. `psycopg2` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ ë°›ì€ response ë¥¼ target DB ì— ì¶”ê°€í•©ë‹ˆë‹¤.

ì•„ë˜ì—ì„œ ìì„¸í•œ ë‚´ìš©ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

### 2.1 í…Œì´ë¸” ìƒì„±

ë¨¼ì €, í…Œì´ë¸” ìƒì„±ì„ í•˜ê² ìŠµë‹ˆë‹¤.

```python
import psycopg2

def create_table(db_connect):
    create_table_query = """
    CREATE TABLE IF NOT EXISTS iris_prediction (
        id SERIAL PRIMARY KEY,
        timestamp timestamp,
        iris_class int
    );"""
    print(create_table_query)
    with db_connect.cursor() as cur:
        cur.execute(create_table_query)
        db_connect.commit()

if __name__ == "__main__":
    db_connect = psycopg2.connect(
        user="targetuser",
        password="targetpassword",
        host="target-postgres-server",
        port=5432,
        database="targetdatabase",
    )
    create_table(db_connect)
```

ì „ë°˜ì ì¸ ì½”ë“œëŠ” `Database` ì±•í„°ì™€ ë™ì¼í•˜ë©° ë‹¤ë¥¸ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- Connection
    - user : `targetuser`
    - password : `targetpassword`
    - host : `target-postgres-server`
    - port : `5432`
    - database : `targetdatabase`
- Table name : `iris_prediction`
- Schema : `id (PK)`, `timestamp (timestamp)`, `iris_class (int)`

### 2.1 Consumer

ë‹¤ìŒìœ¼ë¡œ, consumer ë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.

`kafka-python` íŒ¨í‚¤ì§€ë¥¼ ì´ìš©í•˜ì—¬ `KafkaConsumer` ì˜ instance ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

```python
from json import loads
from kafka import KafkaConsumer

consumer = KafkaConsumer(
    "postgres-source-iris_data",
    bootstrap_servers="broker:29092",
    auto_offset_reset="earliest",
    enable_auto_commit=True,
    group_id="iris",
    value_deserializer=lambda x: loads(x.decode("utf-8")),
)
```

`KafkaConsumer` ì— ë“¤ì–´ê°€ëŠ” ì¸ìë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

- `topics` : ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ì‹¶ì€ topic ë“¤ì„ ë„£ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `postgres-source-iris_data` topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì´ê¸° ë•Œë¬¸ì— `postgres-source-iris_data` ì„ ì ì–´ì¤ë‹ˆë‹¤.
- `bootstrap_servers` : bootstrap server ë¡œ ë„ì›Œì ¸ìˆëŠ” broker ì˜ <broker service name : broker service internal port\> ì„ ë„£ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `broker:29092` ë¥¼ ì ìŠµë‹ˆë‹¤.
- `auto_offset_reset` : topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì–´ë–¤ offset ê°’ë¶€í„° ê°€ì ¸ì˜¬ ì§€ ì„¤ì •í•©ë‹ˆë‹¤. 2ê°€ì§€ ì„¤ì •ì´ ìˆìœ¼ë©°, `earliest` ëŠ” ê°€ì¥ ì´ˆê¸° offset ê°’, `latest` ëŠ” ê°€ì¥ ë§ˆì§€ë§‰ offset ê°’ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì²«ë²ˆì§¸ ë°ì´í„°ë¶€í„° ê°€ì ¸ì˜¤ê³  ì‹¶ê¸° ë•Œë¬¸ì— `earliest` ë¥¼ ì ì–´ì¤ë‹ˆë‹¤.
- `group_id`  : consumer ê·¸ë£¹ì„ ì‹ë³„í•˜ê¸° ìœ„í•´ ê·¸ë£¹ ID ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `iris-data-consumer-group` ë¡œ ì§€ì •í•˜ê² ìŠµë‹ˆë‹¤.
- `value_deserializer` : source connector (ë˜ëŠ” produceer) ì—ì„œ serialization ëœ value ê°’ì„ deserialization í•  ë•Œ ì–´ë–¤ deserializer ë¥¼ ì‚¬ìš©í•  ì§€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì•ì„œ `Kafka` ì±•í„°ì—ì„œ  connect ë¥¼ ë„ìš¸ ë•Œ value converter ë¡œì„œ json converter ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë°ì´í„°ëŠ” json ìœ¼ë¡œ serialization ì´ ë˜ì–´ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ json deserializer ë¥¼ ì´ìš©í•˜ì—¬ deserialization ì„ í•´ì£¼ì–´ì•¼ í•¨ìœ¼ë¡œ, lambda function ê³¼ json ì˜ loads ë¥¼ ì´ìš©í•˜ì—¬ `lambda x: loads(x)` ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.

ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ consumer instance ëŠ” for ë¬¸ì„ ì´ìš©í•˜ì—¬ topic ì— ìˆëŠ” ë°ì´í„°ë¥¼ ê³„ì†í•´ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
for msg in consumer:
    print(
        f"Topic : {msg.topic}\n"
        f"Partition : {msg.partition}\n"
        f"Offset : {msg.offset}\n"
        f"Key : {msg.key}\n"
        f"Value : {msg.value}\n",
    )
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 133
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}}
# 
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 134
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 135, 'timestamp': '2022-12-15 04:49:42.27', 'sepal_length': 6.2, 'sepal_width': 2.9, 'petal_length': 4.3, 'petal_width': 1.3, 'target': 1}}
#
# Topic : postgres-source-iris_data
# Partition : 0
# Offset : 135
# Key : None
# Value : {'schema': {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'field': 'id'}, {'type': 'string', 'optional': True, 'field': 'timestamp'}, {'type': 'double', 'optional': True, 'field': 'sepal_length'}, {'type': 'double', 'optional': True, 'field': 'sepal_width'}, {'type': 'double', 'optional': True, 'field': 'petal_length'}, {'type': 'double', 'optional': True, 'field': 'petal_width'}, {'type': 'int32', 'optional': True, 'field': 'target'}], 'optional': False, 'name': 'iris_data'}, 'payload': {'id': 225, 'timestamp': '2022-12-15 04:51:14.238', 'sepal_length': 6.7, 'sepal_width': 3.1, 'petal_length': 4.4, 'petal_width': 1.4, 'target': 1}}
```

Print ë¬¸ê³¼ ì¶œë ¥ëœ í˜•íƒœë¥¼ í†µí•´ ë©”ì‹œì§€ì— ìˆëŠ” topic, partition, offset, key, value ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„°ëŠ” value ì— ìˆëŠ” `payload` í‚¤ ê°’ì…ë‹ˆë‹¤.

`payload` í‚¤ ê°’ì˜ í˜•íƒœë¥¼ ë³´ë©´ `'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}` ì™€ ê°™ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 2.2 API í˜¸ì¶œ

ë‹¤ìŒ ê³¼ì •ì€ ë°›ì•„ì§„ ë°ì´í„°ë¥¼ `API Serving` ì±•í„°ì—ì„œ ë„ì›Œì§„ API server ì— ì „ë‹¬í•˜ê³  model ì˜ inference ê²°ê³¼ë¥¼ ë°›ëŠ” ê²ƒì…ë‹ˆë‹¤.

`API Serving` ì±•í„°ì—ì„œ ë„ì›Œë‘” API server ì˜ schema ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. 

```python
# schemas.py
from pydantic import BaseModel

class PredictIn(BaseModel):
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float

class PredictOut(BaseModel):
    iris_class: int
```

API server ì— request ë¡œ ì „ë‹¬í•  ê°’ë“¤ì—ëŠ” `sepal_length`, `sepal_width`, `petal_length`, `petal_width` ë§Œ ì „ë‹¬í•´ì•¼í•©ë‹ˆë‹¤. ë”°ë¼ì„œ value ê°’ì˜ payload ì—ì„œ í•„ìš”ì—†ëŠ” column ë“¤ì€ ì‚­ì œë¥¼ í•´ì¤ë‹ˆë‹¤.

```python
msg.value["payload"].pop("id")
msg.value["payload"].pop("target")
ts = msg.value["payload"].pop("timestamp")
```

timestamp ì˜ ê²½ìš°, source DB ì—ì„œ ë‚˜ì˜¨ timestamp ë¥¼ target DB ì— ë„£ì–´ì¤„ ê²ƒì´ê¸° ë•Œë¬¸ì— ë˜‘ê°™ì´ ì‚­ì œëŠ” í•˜ë˜, `ts` ë³€ìˆ˜ë¡œ í• ë‹¹í•´ì¤ë‹ˆë‹¤.

ì´ì œ `requests` íŒ¨í‚¤ì§€ì— ìˆëŠ” POST request ë¥¼ ì´ìš©í•˜ì—¬ payload ê°’ë“¤ì„ ë³´ë‚´ê³  response ë¥¼ ë°›ìŠµë‹ˆë‹¤. 

```python
response = requests.post(
    url="http://api-with-model:8000/predict",
    json=msg.value["payload"],
    headers={"Content-Type": "application/json"},
).json()
response["timestamp"] = ts
```

ë³´ë‚¼ ë•Œ ì“°ì´ëŠ” ì¸ìë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- `url` : API server ì˜ endpoint ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” API server ì˜ host name ê³¼ port number, ê·¸ë¦¬ê³  POST method ì¸ predict ë¥¼ í•©í•˜ì—¬ `"http://api-with-model:8000/predict"` ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.
- `json` : request ë¡œ ë³´ë‚¼ ì¸ìê°’ë“¤ì„ ëª…ì‹œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” value ê°’ì— ìˆëŠ” payload ê°’ì¸ `msg.value["payload"]` ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤.
- `headers` : client ì—ì„œ server ë¡œ request ë¥¼ ë³´ë‚¼ ë•Œ ë¶€ê°€ì ì¸ ì •ë³´ë¥¼ ì „ì†¡í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë³´ë‚¼ ë•Œ json í˜•ì‹ìœ¼ë¡œ ë³´ë‚¼ ê²ƒì´ê¸° ë•Œë¬¸ì— `{"Content-Type": "application/json"}` header ë¡œ ì ì–´ì¤ë‹ˆë‹¤.

Response ë¥¼ ë°›ê³  ë‚œ ë’¤ì— ì•„ê¹Œ ë‚¨ê²¨ë‘ì—ˆë˜ `ts` ë³€ìˆ˜ë¥¼ response ì— ë„£ì–´ì¤ë‹ˆë‹¤.

### 2.3 Target DB ì— ë°ì´í„° ì¶”ê°€

ë§ˆì§€ë§‰ìœ¼ë¡œ `Database` ì±•í„°ì—ì„œ ì‚¬ìš©í–ˆë˜ `insert_data` method ë¥¼ ì´ìš©í•˜ì—¬ response ì— ë‹´ê¸´ ë°ì´í„°ë¥¼ target DB ì— ì¶”ê°€í•©ë‹ˆë‹¤.

```python
def insert_data(db_connect, data):
    insert_row_query = f"""
    INSERT INTO iris_prediction
        (timestamp, iris_class)
        VALUES (
            '{data["timestamp"]}',
            {data["iris_class"]}
        );"""
    print(insert_row_query)
    with db_connect.cursor() as cur:
        cur.execute(insert_row_query)
        db_connect.commit()

insert_data(db_connect, response)
```

### 2.4 ì „ì²´ ì½”ë“œ

ì•ì„œ ì‚´í´ë´¤ë˜ ëª¨ë“  ì½”ë“œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
# data_subscriber.py
from json import loads

import psycopg2
import requests
from kafka import KafkaConsumer

def create_table(db_connect):
    create_table_query = """
    CREATE TABLE IF NOT EXISTS iris_prediction (
        id SERIAL PRIMARY KEY,
        timestamp timestamp,
        iris_class int
    );"""
    print(create_table_query)
    with db_connect.cursor() as cur:
        cur.execute(create_table_query)
        db_connect.commit()

def insert_data(db_connect, data):
    insert_row_query = f"""
    INSERT INTO iris_prediction
        (timestamp, iris_class)
        VALUES (
            '{data["timestamp"]}',
            {data["iris_class"]}
        );"""
    print(insert_row_query)
    with db_connect.cursor() as cur:
        cur.execute(insert_row_query)
        db_connect.commit()

def subscribe_data(db_connect, consumer):
    for msg in consumer:
        print(
            f"Topic : {msg.topic}\n"
            f"Partition : {msg.partition}\n"
            f"Offset : {msg.offset}\n"
            f"Key : {msg.key}\n"
            f"Value : {msg.value}\n",
        )

        msg.value["payload"].pop("id")
        msg.value["payload"].pop("target")
        ts = msg.value["payload"].pop("timestamp")

        response = requests.post(
            url="http://api-with-model:8000/predict",
            json=msg.value["payload"],
            headers={"Content-Type": "application/json"},
        ).json()
        response["timestamp"] = ts
        insert_data(db_connect, response)

if __name__ == "__main__":
    db_connect = psycopg2.connect(
        user="targetuser",
        password="targetpassword",
        host="target-postgres-server",
        port=5432,
        database="targetdatabase",
    )
    create_table(db_connect)

    consumer = KafkaConsumer(
        "postgres-source-iris_data",
        bootstrap_servers="broker:29092",
        auto_offset_reset="earliest",
        group_id="iris-data-consumer-group",
        value_deserializer=lambda x: loads(x),
    )
    subscribe_data(db_connect, consumer)
```

### 2.5 Dockerfile

Data subscriber ì½”ë“œë¥¼ docker ì—ì„œ ì‹¤í–‰í•  `Dockerfile` ì…ë‹ˆë‹¤.

```docker
FROM amd64/python:3.9-slim

WORKDIR /usr/app

RUN pip install -U pip &&\
    pip install psycopg2-binary kafka-python requests

COPY data_subscriber.py data_subscriber.py

ENTRYPOINT ["python", "data_subscriber.py"]
```

### 2.6 Docker-compose

`Dockerfile` ì„ ì´ìš©í•˜ì—¬ docker-compose ë¥¼ êµ¬ì„±í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

```yaml
# stream-docker-compose.yaml
version: "3"

services:
  data-subscriber:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: data-subscriber

networks:
  default:
    name: mlops-network
    external: true
```

### 2.7 ì‹¤í–‰

`docker compose` ëª…ë ¹ì–´ë¥¼ ì´ìš©í•˜ì—¬ data subscriber ì„œë¹„ìŠ¤ë¥¼ ë„ì›ë‹ˆë‹¤.

```bash
$ docker compose -p ch8-stream -f stream-docker-compose.yaml up -d
```

ì´ë•Œ `-p` ì¸ project name ì€ `ch8-stream` ìœ¼ë¡œ ì •í•˜ê³ , `-f` ì¸ file name ì€ ìœ„ì—ì„œ ì‘ì„±í•œ íŒŒì¼ ì´ë¦„ì„ ì ì–´ì¤ë‹ˆë‹¤.

## 3. ë°ì´í„° í™•ì¸

`psql` ë¡œ target DB ì— ì ‘ì†í•©ë‹ˆë‹¤.

```bash
$ PGPASSWORD=targetpassword psql -h localhost -p 5433 -U targetuser -d targetdatabase
psql (14.6 (Homebrew), server 14.0 (Debian 14.0-1.pgdg110+1))
Type "help" for help.

targetdatabase=#
```

Select ë¬¸ì„ ì‘ì„±í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

```bash
targetdatabase=# SELECT * FROM iris_data LIMIT 100;
 id  |        timestamp        | sepal_length | sepal_width | petal_length | petal_width | target 
-----+-------------------------+--------------+-------------+--------------+-------------+--------
   1 | 2022-12-15 05:58:57.024 |          6.4 |         3.2 |          5.3 |         2.3 |      2
   2 | 2022-12-15 05:58:58.047 |          5.2 |         4.1 |          1.5 |         0.1 |      0
   3 | 2022-12-15 05:58:59.063 |          5.7 |         4.4 |          1.5 |         0.4 |      0
   4 | 2022-12-15 05:59:00.07  |          6.5 |           3 |          5.5 |         1.8 |      2
   5 | 2022-12-15 05:59:01.079 |          6.7 |         3.1 |          4.4 |         1.4 |      1
   6 | 2022-12-15 05:59:02.094 |          5.2 |         3.4 |          1.4 |         0.2 |      0
   7 | 2022-12-15 05:59:03.102 |          6.7 |           3 |            5 |         1.7 |      1
   8 | 2022-12-15 05:59:04.127 |          5.5 |         2.3 |            4 |         1.3 |      1
   9 | 2022-12-15 05:59:05.161 |          5.4 |         3.4 |          1.5 |         0.4 |      0
:
```
